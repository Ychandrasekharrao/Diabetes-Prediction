{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3b0a2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Setup complete.\n",
      "Loading processed data from: C:\\Users\\91833\\end to end projects\\diabetes prediction\\data\\processed\\processed_heart_disease.csv\n",
      ">> Data loaded successfully. Shape: (68361, 14)\n",
      ">> Feature encoding complete. Final shape: (68361, 10)\n",
      ">> Train-test split and SMOTETomek completed.\n",
      "   Train shape: (46344, 10), Test shape: (13673, 10)\n",
      "--- Training Logistic Regression ---\n",
      "--- Training XGBoost ---\n",
      "Error training XGBoost: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "--- Training AdaBoost ---\n",
      "--- Training LightGBM ---\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.552043\n",
      "--- Training Random Forest ---\n",
      "--- Training SVM ---\n",
      "\n",
      ">> Model Performance Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.724786</td>\n",
       "      <td>0.745591</td>\n",
       "      <td>0.674346</td>\n",
       "      <td>0.708181</td>\n",
       "      <td>0.791805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.719374</td>\n",
       "      <td>0.752496</td>\n",
       "      <td>0.645695</td>\n",
       "      <td>0.695016</td>\n",
       "      <td>0.783467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.690997</td>\n",
       "      <td>0.706723</td>\n",
       "      <td>0.642741</td>\n",
       "      <td>0.673215</td>\n",
       "      <td>0.757674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.690631</td>\n",
       "      <td>0.702858</td>\n",
       "      <td>0.650126</td>\n",
       "      <td>0.675464</td>\n",
       "      <td>0.752764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.687998</td>\n",
       "      <td>0.685199</td>\n",
       "      <td>0.684389</td>\n",
       "      <td>0.684794</td>\n",
       "      <td>0.746188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1-Score   ROC AUC\n",
       "0             LightGBM  0.724786   0.745591  0.674346  0.708181  0.791805\n",
       "1             AdaBoost  0.719374   0.752496  0.645695  0.695016  0.783467\n",
       "2  Logistic Regression  0.690997   0.706723  0.642741  0.673215  0.757674\n",
       "3                  SVM  0.690631   0.702858  0.650126  0.675464  0.752764\n",
       "4        Random Forest  0.687998   0.685199  0.684389  0.684794  0.746188"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Best Model Selected (based on ROC AUC, then Recall): LightGBM\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SCRIPT: 02 - Model Training, Comparison, and Selection (Notebook Version)\n",
    "# AUTHOR: [Your Name]\n",
    "# DATE:   17-Sep-2025\n",
    "# ==============================================================================\n",
    "\n",
    "# ==================================================\n",
    "# 1. SETUP AND IMPORTS\n",
    "# ==================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Imbalanced-learn imports\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Other libraries\n",
    "import shap\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter display\n",
    "from IPython.display import display\n",
    "\n",
    "# ==================================================\n",
    "# 2. PROJECT ROOT HANDLING (Notebook Safe)\n",
    "# ==================================================\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parent.parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import utility\n",
    "try:\n",
    "    from src.utils.data_loader import load_processed_data\n",
    "except ImportError:\n",
    "    def load_processed_data():\n",
    "        \"\"\"Fallback function to load processed data\"\"\"\n",
    "        processed_data_path = PROJECT_ROOT / 'data' / 'processed' / 'processed_diabetes_data.csv'\n",
    "        if processed_data_path.exists():\n",
    "            return pd.read_csv(processed_data_path)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Config / Paths ---\n",
    "sns.set(style=\"whitegrid\")\n",
    "BEST_MODEL_FIG_DIR = PROJECT_ROOT / 'reports' / 'figures' / 'best_model_analysis'\n",
    "BEST_MODEL_FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\">> Setup complete.\")\n",
    "\n",
    "# ==================================================\n",
    "# 3. LOAD PROCESSED DATA\n",
    "# ==================================================\n",
    "df = load_processed_data()\n",
    "if df.empty:\n",
    "    raise FileNotFoundError(\"Processed data not found. Run preprocessing first.\")\n",
    "else:\n",
    "    print(f\">> Data loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# ==================================================\n",
    "# 4. PREPARE MODELING DATAFRAME\n",
    "# ==================================================\n",
    "X_model = df.drop([\"target\", \"id\",'Systolic_BP', 'Diastolic_BP'], axis=1).copy()\n",
    "y = df[\"target\"].copy()\n",
    "\n",
    "# --- Encoding ---\n",
    "ordinal_cols = [\"Cholesterol_Level\", \"Glucose_Level\", \"Smoking_Status\",\n",
    "                \"Alcohol_Intake\", \"Physical_Activity\", \"BP_level\"]\n",
    "nominal_cols = [\"Sex\"]\n",
    "\n",
    "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "X_model[ordinal_cols] = encoder.fit_transform(X_model[ordinal_cols])\n",
    "X_model.rename(columns={col: f\"ordinal_{col}\" for col in ordinal_cols}, inplace=True)\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown=\"ignore\")\n",
    "encoded_features = ohe.fit_transform(X_model[nominal_cols])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=ohe.get_feature_names_out(nominal_cols), index=X_model.index)\n",
    "X_model = X_model.drop(nominal_cols, axis=1)\n",
    "X_model = pd.concat([X_model, encoded_df], axis=1)\n",
    "\n",
    "print(\">> Feature encoding complete. Final shape:\", X_model.shape)\n",
    "\n",
    "# ==================================================\n",
    "# 5. TRAIN-TEST SPLIT AND SMOTETOMEK\n",
    "# ==================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_model, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "smote_tomek = SMOTETomek(sampling_strategy='auto', random_state=42)\n",
    "X_train_res, y_train_res = smote_tomek.fit_resample(X_train, y_train)\n",
    "print(\">> Train-test split and SMOTETomek completed.\")\n",
    "print(f\"   Train shape: {X_train_res.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# ==================================================\n",
    "# 6. DEFINE GENERIC TRAINING FUNCTION WITH EARLY STOPPING\n",
    "# ==================================================\n",
    "def train_with_early_stopping(model, X_train, y_train, X_val, y_val, early_stopping_rounds=20, verbose=True):\n",
    "    model_name = type(model).__name__\n",
    "\n",
    "    # XGBoost\n",
    "    if model_name == \"XGBClassifier\":\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=verbose if verbose else 0\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # LightGBM\n",
    "    elif model_name == \"LGBMClassifier\":\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=verbose)]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # SVM large dataset sampling\n",
    "    elif model_name == \"SVC\":\n",
    "        if len(X_train) > 10000:\n",
    "            sample_size = min(10000, len(X_train))\n",
    "            idx = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "            X_sample, y_sample = X_train.iloc[idx], y_train.iloc[idx]\n",
    "            model.fit(X_sample, y_sample)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    # Other models\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "# ==================================================\n",
    "# 7. DEFINE MODELS (no class_weight, use SMOTETomek only)\n",
    "# ==================================================\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(eval_metric=\"logloss\", random_state=42, verbosity=0, n_jobs=-1, n_estimators=200),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42, n_estimators=200),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=200, random_state=42, n_jobs=-1, verbosity=-1),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# ==================================================\n",
    "# 8. TRAIN MODELS\n",
    "# ==================================================\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Training {name} ---\")\n",
    "    try:\n",
    "        trained_model = train_with_early_stopping(model, X_train_res, y_train_res, X_test, y_test)\n",
    "        trained_models[name] = trained_model\n",
    "\n",
    "        y_pred = trained_model.predict(X_test)\n",
    "\n",
    "        try:\n",
    "            if hasattr(trained_model, \"predict_proba\"):\n",
    "                y_pred_proba = trained_model.predict_proba(X_test)[:, 1]\n",
    "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            else:\n",
    "                y_pred_proba = np.nan\n",
    "                roc_auc = np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate probabilities for {name}. Error: {str(e)}\")\n",
    "            y_pred_proba = np.nan\n",
    "            roc_auc = np.nan\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'F1-Score': f1_score(y_test, y_pred),\n",
    "            'ROC AUC': roc_auc\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# ==================================================\n",
    "# 9. COMPARE MODELS AND SELECT BEST (ROC AUC first, then Recall)\n",
    "# ==================================================\n",
    "results_df = pd.DataFrame(results).sort_values(\n",
    "    by=['ROC AUC', 'Recall'], ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n>> Model Performance Summary:\")\n",
    "display(results_df)\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"\\n>> Best Model Selected (based on ROC AUC, then Recall): {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ddcf061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for the Best Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.77      0.74      6902\n",
      "           1       0.75      0.67      0.71      6771\n",
      "\n",
      "    accuracy                           0.72     13673\n",
      "   macro avg       0.73      0.72      0.72     13673\n",
      "weighted avg       0.73      0.72      0.72     13673\n",
      "\n",
      "Confusion matrix plot saved.\n",
      "ROC AUC curve plot saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "# --- A. Classification Report ---\n",
    "y_pred_best = trained_models[best_model_name].predict(X_test)\n",
    "print(\"Classification Report for the Best Model:\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Get probability predictions for ROC curve\n",
    "y_pred_proba_best = trained_models[best_model_name].predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- B. Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])\n",
    "plt.title(f'Confusion Matrix for {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.savefig(BEST_MODEL_FIG_DIR / '1_confusion_matrix.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Confusion matrix plot saved.\")\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_best)\n",
    "auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title(f'ROC Curve for {best_model_name}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(BEST_MODEL_FIG_DIR / '2_roc_auc_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"ROC AUC curve plot saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba5d09c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91833\\end to end projects\\diabetes prediction\\env\\Lib\\site-packages\\shap\\explainers\\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating SHAP values with TreeExplainer...\n",
      "SHAP summary plot saved.\n"
     ]
    }
   ],
   "source": [
    "# --- D. SHAP Analysis ---\n",
    "# UPDATE: Added LGBM and AdaBoost to the TreeExplainer check\n",
    "if isinstance(trained_models[best_model_name], (XGBClassifier, RandomForestClassifier, LGBMClassifier, AdaBoostClassifier)):\n",
    "    explainer = shap.TreeExplainer(trained_models[best_model_name])\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    print(\"Calculating SHAP values with TreeExplainer...\")\n",
    "else:\n",
    "    explainer = shap.KernelExplainer(trained_models[best_model_name].predict_proba, shap.sample(X_train_res, 100))\n",
    "    shap_values = explainer.shap_values(X_test)[1]\n",
    "    print(\"Calculating SHAP values with KernelExplainer (this may be slow)...\")\n",
    "\n",
    "shap.summary_plot(shap_values, X_test, show=False)\n",
    "plt.title(f\"SHAP Summary for {best_model_name}\")\n",
    "plt.savefig(BEST_MODEL_FIG_DIR / '3_shap_summary.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"SHAP summary plot saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "493b4785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "8. Saving the Best Model\n",
      "==================================================\n",
      " Best model saved to: C:\\Users\\91833\\end to end projects\\diabetes prediction\\models\\best_model_lightgbm.joblib\n",
      "\n",
      "Model Training Script Finished Successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# 8. SAVE THE BEST MODEL\n",
    "# ==================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"8. Saving the Best Model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model_path = MODELS_DIR / f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.joblib'\n",
    "joblib.dump(best_model_name, model_path)\n",
    "print(f\" Best model saved to: {model_path}\")\n",
    "\n",
    "print(\"\\nModel Training Script Finished Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ed871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
