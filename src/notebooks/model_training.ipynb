{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0791e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils/data_loader.py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the data directory path\n",
    "DATA_DIR = Path('../data')\n",
    "PROCESSED_DIR = DATA_DIR\n",
    "\n",
    "def load_processed_data(filename=\"heart_disease_processed.csv\"):\n",
    "    \"\"\"\n",
    "    Load the processed CSV for modeling.\n",
    "    \"\"\"\n",
    "    file_path = PROCESSED_DIR / filename\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Processed file not found: {file_path}\")\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34673ae7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'processed_heart_disease.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprocessed_heart_disease.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     results_df, best_model_name = train_evaluate(df)\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal Model Comparison:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91833\\end to end projects\\dibetes prediction\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91833\\end to end projects\\dibetes prediction\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91833\\end to end projects\\dibetes prediction\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91833\\end to end projects\\dibetes prediction\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91833\\end to end projects\\dibetes prediction\\env\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'processed_heart_disease.csv'"
     ]
    }
   ],
   "source": [
    "# extended_model_pipeline_best_shap.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# -------------------------\n",
    "# 1. Preprocess & Split\n",
    "# -------------------------\n",
    "def preprocess_split(df, target_col=\"target\"):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].mode()[0] if df[col].dtype == \"int\" else df[col].median()\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].astype(int)\n",
    "    return train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Build Pipeline\n",
    "# -------------------------\n",
    "def build_pipeline(model_name=\"LogisticRegression\"):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    if model_name == \"LogisticRegression\":\n",
    "        model = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "    elif model_name == \"RandomForest\":\n",
    "        model = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=42)\n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "    elif model_name == \"LightGBM\":\n",
    "        model = LGBMClassifier(random_state=42)\n",
    "    elif model_name == \"AdaBoost\":\n",
    "        model = AdaBoostClassifier(random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    pipeline = ImbPipeline([\n",
    "        (\"balancer\", SMOTE(random_state=42)),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# -------------------------\n",
    "# 3. Train & Evaluate Models\n",
    "# -------------------------\n",
    "def train_evaluate(df, models=None, target_col=\"target\"):\n",
    "    if models is None:\n",
    "        models = [\"LogisticRegression\",\"RandomForest\",\"XGBoost\",\"LightGBM\",\"AdaBoost\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = preprocess_split(df, target_col)\n",
    "    results = []\n",
    "    pipelines = {}\n",
    "    \n",
    "    # Train all models\n",
    "    for m in models:\n",
    "        pipe = build_pipeline(m)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        pipelines[m] = pipe\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        metrics = {\n",
    "            \"Model\": m,\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"F1\": f1_score(y_test, y_pred),\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"Precision\": precision_score(y_test, y_pred)\n",
    "        }\n",
    "        results.append(metrics)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Identify best model (highest F1)\n",
    "    best_model_name = results_df.sort_values(\"F1\", ascending=False).iloc[0][\"Model\"]\n",
    "    print(f\"\\nBest Model based on F1: {best_model_name}\")\n",
    "    \n",
    "    # SHAP for best model (if tree-based)\n",
    "    if best_model_name in [\"RandomForest\", \"XGBoost\", \"LightGBM\"]:\n",
    "        best_pipe = pipelines[best_model_name]\n",
    "        model = best_pipe.named_steps[\"clf\"]\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap_importance = pd.Series(np.abs(shap_values).mean(axis=0), index=X_train.columns)\n",
    "        print(f\"\\nSHAP Feature Importance for {best_model_name}:\\n\", shap_importance.sort_values(ascending=False))\n",
    "    \n",
    "    return results_df, best_model_name\n",
    "\n",
    "# -------------------------\n",
    "# Example Usage\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"processed_heart_disease.csv\")\n",
    "    results_df, best_model_name = train_evaluate(df)\n",
    "    print(\"\\nFinal Model Comparison:\")\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9396a9b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "The file ..\\data\\processed_heart_disease.csv does not exist. Please ensure the processed dataset is in the correct location.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    128\u001b[39m csv_path = PROCESSED_DIR / \u001b[33m\"\u001b[39m\u001b[33mprocessed_heart_disease.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m csv_path.exists():\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m    131\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist. Please ensure the processed dataset is in the correct location.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m     )\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[32m    135\u001b[39m df = pd.read_csv(\u001b[38;5;28mstr\u001b[39m(csv_path))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: The file ..\\data\\processed_heart_disease.csv does not exist. Please ensure the processed dataset is in the correct location."
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Notebook Modeling Pipeline\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "\n",
    "# -------------------------\n",
    "# 1. Preprocess & split\n",
    "# -------------------------\n",
    "def preprocess_split(df, target_col=\"target\"):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].mode()[0] if df[col].dtype == \"int\" else df[col].median()\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].astype(int)\n",
    "    return train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Build pipeline\n",
    "# -------------------------\n",
    "def build_pipeline(model_name=\"LogisticRegression\"):\n",
    "    scaler = StandardScaler()\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        model = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "    elif model_name == \"LightGBM\":\n",
    "        model = LGBMClassifier(random_state=42, verbose=-1)  # Added verbose=-1 to suppress warnings\n",
    "    elif model_name == \"RandomForest\":\n",
    "        model = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=42)\n",
    "    elif model_name == \"AdaBoost\":\n",
    "        model = AdaBoostClassifier(random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model\")\n",
    "    \n",
    "    pipeline = ImbPipeline([\n",
    "        (\"balancer\", SMOTE(random_state=42)),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# -------------------------\n",
    "# 3. Train & evaluate\n",
    "# -------------------------\n",
    "def train_evaluate(df, models=[\"LogisticRegression\",\"XGBoost\",\"LightGBM\",\"RandomForest\",\"AdaBoost\"]):\n",
    "    X_train, X_test, y_train, y_test = preprocess_split(df)\n",
    "    results = []\n",
    "    best_model_name = None\n",
    "    best_f1 = 0\n",
    "    best_pipeline = None\n",
    "\n",
    "    for m in models:\n",
    "        print(f\"Training {m}...\")\n",
    "        pipe = build_pipeline(m)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        metrics = {\n",
    "            \"Model\": m,\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"F1\": f1,\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"Precision\": precision_score(y_test, y_pred)\n",
    "        }\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # track best model for SHAP\n",
    "        if f1 > best_f1 and m in [\"XGBoost\",\"LightGBM\",\"RandomForest\",\"AdaBoost\"]:\n",
    "            best_f1 = f1\n",
    "            best_model_name = m\n",
    "            best_pipeline = pipe\n",
    "\n",
    "    # SHAP only for best tree-based model\n",
    "    if best_pipeline is not None:\n",
    "        print(f\"Generating SHAP analysis for {best_model_name}...\")\n",
    "        # Get the transformed training data for SHAP\n",
    "        X_train_transformed = best_pipeline.named_steps[\"balancer\"].fit_resample(X_train, y_train)[0]\n",
    "        X_train_scaled = best_pipeline.named_steps[\"scaler\"].transform(X_train_transformed)\n",
    "        \n",
    "        # Get the transformed test data\n",
    "        X_test_scaled = best_pipeline.named_steps[\"scaler\"].transform(X_test)\n",
    "        \n",
    "        model = best_pipeline.named_steps[\"clf\"]\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test_scaled)\n",
    "        \n",
    "        # Handle multi-class case (if shap_values is a list)\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]  # Use positive class for binary classification\n",
    "        \n",
    "        shap_importance = pd.Series(np.abs(shap_values).mean(axis=0), index=X_train.columns)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap_importance.sort_values().plot(kind=\"barh\", color=\"skyblue\")\n",
    "        plt.title(f\"SHAP Feature Importance ({best_model_name})\")\n",
    "        plt.xlabel(\"Mean |SHAP Value|\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return pd.DataFrame(results), best_model_name\n",
    "\n",
    "# -------------------------\n",
    "# USAGE\n",
    "# -------------------------\n",
    "# Create directory if it doesn't exist\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if file exists\n",
    "csv_path = PROCESSED_DIR / \"processed_heart_disease.csv\"\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"The file {csv_path} does not exist. Please ensure the processed dataset is in the correct location.\"\n",
    "    )\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(str(csv_path))\n",
    "\n",
    "# Run the modeling pipeline\n",
    "results_df, best_model = train_evaluate(df)\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "print(results_df.round(4))\n",
    "print(f\"\\nBest Model for SHAP: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22650dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
